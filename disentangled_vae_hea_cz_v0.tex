\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}

\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\urlstyle{tt}
\newcommand{\email}[1]{\href{mailto:#1}{\tt{\nolinkurl{#1}}}}
\newcommand{\orcid}[1]{ORCID: \href{https://orcid.org/#1}{\tt{\nolinkurl{#1}}}}

\usepackage[sfdefault,lf]{carlito}
%% The 'lf' option for lining figures
%% The 'sfdefault' option to make the base font sans serif
\usepackage[parfill]{parskip}
\renewcommand*\oldstylenums[1]{\carlitoOsF #1}
\usepackage{fancyhdr}
% \usepackage{natbib}
\usepackage{authblk}
\setlength{\headheight}{41pt}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{color}
\usepackage{float}

\usepackage[colorinlistoftodos]{todonotes}

\newcommand{\cz}[1]{\hl{\textbf{cz:} #1}}




%\renewcommand{\headrulewidth}{0pt}
% \fancyhead[L]{Posted: \today}
% \fancyhead[R]{
% \includegraphics[width=4cm]{./figures/engrXiv_banner.png}
% }
\pagestyle{plain}
\title{Data-efficient and interpretable inverse materials design using a disentangled variational autoencoder}
\author[1, $\dag$, *]{Zulqarnain Khan}
\author[1, 2, $\dag$, *]{Cheng Zeng}
\author[2]{Nathan L. Post}
\affil[1]{Institute for Experiential AI, Northeastern University, Boston, MA 02115, United States}
\affil[2]{The Roux Institute, Northeastern University, Portland, ME 04101, United States}
\affil[$\dag$]{These authors contribute equally: Zulqarnain Khan, Cheng Zeng.}
\affil[*]{Corresponding author: \email{z.khan@northeastern.edu}, \email{c.zeng@northeastern.edu}}
\date{}

\setlength {\marginparwidth }{2cm}

\begin{document}
\maketitle
\thispagestyle{fancy}

% \cz{I removed the banner header as we usually submit Materials design work to arXiv rather than EngrXiv.}

% \cz{Added the author info, but we can discuss...}

\begin{abstract}
Your abstract.
\end{abstract}

\section{Introduction}

%%%% Materials importance and Materials design history
Materials play a pivotal role in shaping the modern society and many grand technology challenges are in essence materials challenges, ranging from lower-cost battery materials for energy storage, to quantum computing materials and bio-compatible materials for healthcare applications~\cite{li2017,de2021,tahmasebi2020}.
Thanks to advances in high-throughput computing~\cite{merchant2023}, robotics~\cite{szymanski2023}, machine learning force fields~\cite{merchant2023} and materials open datasets~\cite{jain2013, choudhary2020}, materials design and discovery have now reached an unprecedented rate and scale~\cite{knapp2022}.
Although advances in algorithm and hardware significantly reduce the computation time for each iteration, it can still take extensive iterations or computation to pinpoint a small range of potential materials candidates with desired properties~\cite{zeng2024}.

%%%% Inverse materials design, its state-of-the-art, limitations
Inverse materials design unlock the potential to optimize new materials towards a target property.
In general there are three approaches for inverse materials design~\cite{wang2022}, including high-throughput virtual screening~\cite{afzal2019}, global optimization~\cite{geng2019}, reinforcement learning~\cite{xian2024} and generative models~\cite{ma2019, popova2018}.
Generative models are probably the most efficient approach as it learns a compact low-dimensional representation and can generates new data using the latent-space encoded knowledge.
Comparing various generative models, a study by T\"{u}rk et al showed that variational autoencoder (VAE) is more robust than reinforcement learning and generative adversarial networks because VAE has a better representation of underlying distributions and training a VAE model is easier~\cite{turk2022}.
However, current generative models are primarily present as an unsupervised learning approach which learns a latent space assumed to entangle the relationship between materials representations and target properties~\cite{chen2020, wang2022, rao2022}.
It is not ideal as one will need to perform post-optimization to explore materials with better target properties and may even fail to find any useful materials.
A recent work by Xie and Tomioka et al introduced diffusion-based generative processes together with a fine tuning process to discover materials with multiple target properties such as magnetic property and supply chain risk~\cite{zeni2024}.
Although a state-of-the-art discovery rate and materials stability were reported, the full periodicity of the crystal structures restricts the design space for inorganic materials, the complexity of large datasets and diffusion models hinder a wider application and the interpretability of the methods, and the practicality of the new materials discovered in this process is doubtful without an expert insight~\cite{cheetham2024}.

%%% The need and summary of our work
Therefore, there is an urgent need to create a workflow for inverse materials design that is data efficient, interpretable and can be geared towards multi-property optimization.
Here the goal is to develop high-entropy alloys (HEAs) which tend to form a single-phase structure.
Conventional methods to predict single-phase alloys rely on sophisticated design of experiments, thermodynamic modeling and first-principles calculations, which are not efficient search for high-entropy alloys  because the number of points to search grow combinatorially with the increase of elements~\cite{yao2017, feng2021}.
For example, a HEA with five possible elements and each element has an integer composition between 5\% and 35\%, the number of possible compositions is on the order of 10$^7$, let along the massive numbers of possible atomic arrangement for each composition.
Although machine learning algorithms have proven to speed up the search of corrosion-resistance or mechanically strong HEAs~\cite{zeng2024, yan2021, tandoc2023}, this forward design strategy still needs to scan a wide configuration and composition space.
Inverse design is theorized to be a much more robust approach as it learns a probabilistic relationship between materials representation and a compact latent space where new materials candidates can be generated from this relationship.
Coupled with an uncertainty estimate, search directions for new materials can be identified with a risk quantification~\cite{rao2022}.
Here we introduced a disentangled generative model using a semi-supervised variational autoencoder for the inverse design of complex materials.
We demonstrated this approach on single-phase high-entropy alloys.
Although our demonstration focuses on single materials property (single phase formation), the approach can also be applied to materials discovery with multi-objective targets.
This approach can utilize both labelled and unlabelled data sets. The proposed methods are highly versatile and robust, and can be readily extended to other engineering domains where there exists a probabilistic structure/composition-property relationship.

\section{Methods \& Theories}
\subsection{Data set}

\begin{figure}
\centering
\includegraphics[width=5.in]{figures/dataset.pdf}
\caption{Experimental high-entropy alloy dataset for single phase formation.
\label{fig:dataset}}
\end{figure}

This work uses an experimental data set where inputs are chemical compositions of alloys and outputs are binary phase predictions indicating the formation of single phase (\textbf{SP}: 1) versus multiple phases (\textbf{MP}: 0).
A summary of this dataset is shown in Figure~\ref{fig:dataset}.
It is an up-to-date and well-recognized dataset for HEA single phase prediction collected by Yan et al~\cite{yan2021}.
Empirical rules suggest that the single phase formation relies more on the synergistic effects of mixing different alloys rather than the independent attributes of each element~\cite{yan2021, pei2020}.
Therefore, eight engineered features are used rather than element-wise compositions.
The eight features include bulk modulus, molar volume, melting temperature, valence electron concentration, atomic size difference, Pauling electronegativity difference, mixing entropy and mixing enthalpy.
Methods to calculate these features can be found in the supporting information of our previous work~\cite{zeng2024}.
Those eight features were found to be informative for the prediction of single phase formation~\cite{zeng2024, yan2021}.
More importantly, this feature engineering provides a general and compact representation of alloys comprising various elements, offering more generalizability for alloy design.
Given that an experimental data set is used, the trained ML models can thus give predictions that are more likely to be manufactured in practice, without explicitly considering the manufacturing process and environmental conditions.


\subsection{Proposed Model: Disentangled VAE}

\subsection{Generative model}
\begin{figure}[H]
    \centering
    \includegraphics[width=2.2in]{figures/HEA_Graphical_Model_Generative_Phase.pdf}
    \includegraphics[width=2.5in]{figures/HEA_Graphical_Model_Recognition_Phase.pdf}
    \caption{Caption}
    \label{fig:vae_model}
\end{figure}

Assume $x \in \mathbb{R}^+$ represents an alloy and that each alloy has a Phase $\phi \in \{0, 1\}$ that we are interested in being able to predict. The Phase is assumed known for at least a subset of the training data. In addition assume $z \in \mathbb{R}^l$ as an  $l$- dimensional latent variable, represents any other factors that may be responsible for generating alloys. With this notation in place, we write the generative model (Figure 4, left) for the data in the form of the following joint probability distribution:
\begin{equation}
    p_\theta(x, \phi, z) = p_\theta(x| \phi, z)p(\phi)p(z)
\end{equation}
Where $\theta$ parameterizes the likelihood using a neural network that takes as input the sampled instances of $\phi$ and $z$. While different choices of priors can be experimented with for each variable, we propose one possible set of choices that best suit the data-space that each variable belongs to. The likelihood of the compositional data is modeled as a multinomial parameterized by the neural network :
\begin{equation}
    x \sim Multinomial(\theta(\phi, z))
\end{equation}

Priors for the binary Phase $\phi$ are modeled using Bernoulli distribution and lastly, we assume a standard normal prior over the latent variable z as an uninformative choice for the same reasons.

\begin{equation}
    \phi \sim Bernoulli(r), z \sim Normal(0, I)
\end{equation}

While informed choices for the hyperparameters of these prior distributions can be made, starting with relatively non-informative choices  avoids introducing bias into the model. The phase prior can be chosen to bias the model towards observing more multiphase alloys vs single phase alloys, as that tends to be true in nature.

\subsubsection{Recognition model}

With the above generative model in place, we can now define the recognition model to perform inference. There are different acceptable recognition models given the above generative model. We propose one that conforms best to the our goals of enabling prediction, interpretation, and exploration. The recognition model $q_\psi(\phi, z | x)$  takes as input the data and maps it to the latent representation space, serving as the variational approximation to the otherwise intractable posterior distribution $p(\phi, z | x)$. We assume the recognition model factorizes as follows under a mean-field assumption:

\begin{equation}
    q(\phi, z | x)=q_{\psi_\phi}(\phi | f(x)) q_{\psi_z}(z|x, \phi)
\end{equation}

The idea behind this factorization is that, we know from prior knowledge that the Phase $\phi$ is well predicted by a physics informed hand-engineered transformation $f(x)$ [8]. Specifically this transformation takes in composition and outputs eight relevant physical descriptors [17]; atomic size difference, mixing enthalpy, mixing entropy, Pauli electronegativity difference, molar volume, bulk modulus, melting temperature and valence electron concentration. In turn the neural network $\psi_\phi$ uses these engineered features to predict the binary Phase. Note that $f(x)$ is a pre-specified transformation and not a learnable function. Finally the latent variable $z$ encodes everything else about the alloy $x$ conditioned upon values of phase. This recognition model allows us to encode hand-engineered features that we know are useful for predicting phase, and rely on the expressivity of neural networks to predict the appropriate values for energy properties as well as encode other information necessary for the eventual reconstruction of the training data through the decoder.


\subsubsection{Model training}
 We can train both the generative model and the recognition model simultaneously by maximizing the following variational objective function w.r.t the neural network parameters for the generative and recognition model:
\begin{equation}
    \sum_{n=1}^N \mathcal{L}(\theta, \psi_z, x^n) + \gamma \sum_{m=1}^M \mathcal{L}^\textit{sup}(\theta, \psi_\phi, \psi_z; x^m, \phi^m)
\end{equation}

The first part of this objective function optimizes over all $n={1, \ldots, N}$ data points for which supervision is not available. Here ={, Es, Esf, z}.  This is the standard VAE evidence lower bound (ELBO) loss, which can generally be thought of as learning to reconstruct input data with some regularization on the latent space. The second term is the supervised loss and uses points where supervision is available i.e. points for which values for phase, surface energy, and stacking fault energy are available
��sup(, ; xm, ym) = Eqz(z | xm, ym)[logp(xm, ym, z)q(ym | xm)qz(z|ym, xm)] + (1+) log q(ym|xm)
Where the constant  is a hyperparameter that balances between optimizing for prediction accuracy and reconstruction accuracy. The objective above can then be approximated using a Monte Carlo estimator as explained in [41]. We will utilize Pyro [42], a pytorch based probabilistic programming language, for model specification as well as training and inference.

\subsubsection{Model deliverables}


\subsection{Post-hoc analysis: SHAP feature importance}

\section{Results and Discussion}

\subsection{Trained recognition and generative models.}

\subsection{Latent space exploration}

\subsection{Latent space clustering?}

\subsection{Rationalizing identified new materials}

\section{Conclusion and Outlook}

\begin{enumerate}
    \item[1] Novel disentangled model
    \item[2] More interpretable inverse design because ...
    \item[3] More data efficient because ...
    \item[4] Reliable new materails and easy to be improved with further computational/experimental validation?

\end{enumerate}

\bibliographystyle{bibstyle}
\bibliography{references}

\end{document}